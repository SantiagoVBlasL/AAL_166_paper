{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2719512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01:15:41 │ INFO     │ Cargando tensor global desde: /home/diego/Escritorio/AAL3_paper/AAL3_dynamicROIs_fmri_tensor_NeuroEnhanced_v6.5.17_AAL3_131ROIs_OMST_GCE_Signed_GrangerLag1_ChNorm_ROIreorderedYeo17_ParallelTuned/GLOBAL_TENSOR_from_AAL3_dynamicROIs_fmri_tensor_NeuroEnhanced_v6.5.17_AAL3_131ROIs_OMST_GCE_Signed_GrangerLag1_ChNorm_ROIreorderedYeo17_ParallelTuned.npz\n",
      "01:15:41 │ INFO     │ Tensor global cargado. Forma: (431, 6, 131, 131). Canales: ['Pearson_OMST_GCE_Signed_Weighted', 'Pearson_Full_FisherZ_Signed', 'MI_KNN_Symmetric', 'dFC_AbsDiffMean', 'dFC_StdDev', 'Granger_F_lag1']\n",
      "01:15:41 │ INFO     │ Cargando metadatos desde: /home/diego/Escritorio/AAL3_paper/SubjectsData_Schaefer2018_400ROIs.csv\n",
      "01:15:41 │ INFO     │ Metadatos cargados. Forma: (434, 24)\n",
      "01:15:41 │ INFO     │ Usando 3 canales: ['Pearson_Full_FisherZ_Signed', 'MI_KNN_Symmetric', 'dFC_AbsDiffMean']\n",
      "01:15:41 │ INFO     │ Total de sujetos CN/AD para clasificación: 184. (CN: 89, AD: 95)\n",
      "01:15:41 │ INFO     │ Usando StratifiedKFold (5 iteraciones).\n",
      "01:15:41 │ INFO     │ --- Iniciando Fold 1/5 ---\n",
      "01:15:41 │ INFO     │ Aplicando normalización inter-canal (modo: zscore_offdiag) sobre 3 canales seleccionados.\n",
      "01:15:41 │ INFO     │ Parámetros de normalización se calcularán usando 334 sujetos de entrenamiento.\n",
      "01:15:41 │ INFO     │ Canal 'Pearson_Full_FisherZ_Signed': Off-diag zscore_offdiag (train_params: mean=-0.046, std=0.770)\n",
      "01:15:41 │ INFO     │ Canal 'MI_KNN_Symmetric': Off-diag zscore_offdiag (train_params: mean=0.058, std=0.812)\n",
      "01:15:42 │ INFO     │ Canal 'dFC_AbsDiffMean': Off-diag zscore_offdiag (train_params: mean=-0.036, std=0.769)\n",
      "/home/diego/anaconda3/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "01:15:43 │ INFO     │   Fold 1/5 VAE Ep   1 | LR 1.00e-04 | β 0.000 | Train Recon 64078.66  KLD   187.72 | Val Loss 53365.10\n",
      "01:15:50 │ INFO     │   Fold 1/5 VAE Ep  21 | LR 1.00e-04 | β 0.121 | Train Recon 41431.34  KLD  1174.73 | Val Loss 39821.20\n",
      "01:15:57 │ INFO     │   Fold 1/5 VAE Ep  41 | LR 1.00e-04 | β 0.242 | Train Recon 36254.56  KLD  1116.49 | Val Loss 35221.34\n",
      "01:16:03 │ INFO     │   Fold 1/5 VAE Ep  61 | LR 1.00e-04 | β 0.364 | Train Recon 33995.19  KLD  1019.20 | Val Loss 33261.58\n",
      "01:16:33 │ INFO     │   Fold 1/5 VAE Ep  81 | LR 1.00e-04 | β 0.485 | Train Recon 32538.48  KLD   933.98 | Val Loss 32192.68\n",
      "01:16:40 │ INFO     │   Fold 1/5 VAE Ep 101 | LR 1.00e-04 | β 0.606 | Train Recon 31622.20  KLD   872.89 | Val Loss 31459.81\n",
      "01:16:46 │ INFO     │   Fold 1/5 VAE Ep 121 | LR 1.00e-04 | β 0.727 | Train Recon 30960.68  KLD   818.79 | Val Loss 31002.27\n",
      "01:16:59 │ INFO     │   Fold 1/5 VAE Ep 141 | LR 1.00e-04 | β 0.848 | Train Recon 30396.88  KLD   775.13 | Val Loss 30760.61\n",
      "01:17:20 │ INFO     │   Fold 1/5 VAE Ep 161 | LR 1.00e-04 | β 0.970 | Train Recon 29992.40  KLD   734.72 | Val Loss 30385.46\n",
      "01:17:25 │ INFO     │   Fold 1/5 VAE Ep 181 | LR 1.00e-04 | β 1.091 | Train Recon 29526.87  KLD   700.59 | Val Loss 30166.56\n",
      "01:17:29 │ INFO     │   Fold 1/5 VAE Ep 201 | LR 1.00e-04 | β 1.212 | Train Recon 29170.45  KLD   664.84 | Val Loss 30116.45\n",
      "01:17:34 │ INFO     │   Fold 1/5 VAE Ep 221 | LR 1.00e-04 | β 1.333 | Train Recon 28916.16  KLD   629.67 | Val Loss 29955.07\n",
      "01:17:40 │ INFO     │   Fold 1/5 VAE Ep 241 | LR 1.00e-04 | β 1.455 | Train Recon 28646.14  KLD   614.03 | Val Loss 29841.75\n",
      "01:17:48 │ INFO     │   Fold 1/5 VAE Ep 261 | LR 1.00e-04 | β 1.500 | Train Recon 28387.06  KLD   607.34 | Val Loss 29779.94\n",
      "01:18:07 │ INFO     │   Fold 1/5 VAE Ep 281 | LR 1.00e-04 | β 0.030 | Train Recon 27839.59  KLD  1289.74 | Val Loss 28598.01\n",
      "01:18:12 │ INFO     │   Fold 1/5 VAE Ep 301 | LR 1.00e-04 | β 0.152 | Train Recon 27609.85  KLD  1250.54 | Val Loss 28613.16\n",
      "01:18:15 │ INFO     │   Fold 1/5 VAE Ep 321 | LR 1.00e-05 | β 0.273 | Train Recon 27575.74  KLD  1129.84 | Val Loss 28622.47\n",
      "01:18:18 │ INFO     │   Fold 1/5 Early stopping VAE en epoch 338.\n",
      "/home/diego/Escritorio/AAL3_paper/sat_night.py:676: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  vae_fold_k.load_state_dict(torch.load(fold_output_dir / \"best_vae_model.pt\"))\n",
      "01:18:19 │ INFO     │   Fold 1/5 Extrayendo features latentes con el VAE entrenado.\n",
      "01:18:19 │ INFO     │   --- Fold 1/5, Clasificador: rf ---\n",
      "01:18:19 │ INFO     │ Ajustando HPs para rf con Optuna (150 trials)...\n",
      "01:18:41 │ INFO     │ Mejores HPs encontrados para rf: {'n_estimators': 59, 'max_depth': 6, 'min_samples_split': 4, 'min_samples_leaf': 9}\n",
      "01:18:42 │ INFO     │     Resultados rf: AUC=0.8509, Bal. Acc.=0.7266\n",
      "01:18:42 │ INFO     │   --- Fold 1/5, Clasificador: gb ---\n",
      "01:18:42 │ INFO     │ Ajustando HPs para gb con Optuna (150 trials)...\n",
      "01:22:24 │ INFO     │ Mejores HPs encontrados para gb: {'n_estimators': 158, 'learning_rate': 0.032797898226514854, 'max_depth': 4}\n",
      "01:22:26 │ INFO     │     Resultados gb: AUC=0.7836, Bal. Acc.=0.6754\n",
      "01:22:26 │ INFO     │   --- Fold 1/5, Clasificador: svm ---\n",
      "01:22:26 │ INFO     │ Ajustando HPs para svm con GridSearchCV...\n",
      "01:22:27 │ INFO     │ Mejores HPs encontrados para svm: {'C': 1, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "01:22:27 │ INFO     │     Resultados svm: AUC=0.8743, Bal. Acc.=0.7836\n",
      "01:22:27 │ INFO     │   --- Fold 1/5, Clasificador: logreg ---\n",
      "01:22:27 │ INFO     │ Ajustando HPs para logreg con GridSearchCV...\n",
      "01:22:27 │ INFO     │ Mejores HPs encontrados para logreg: {'C': 10}\n",
      "01:22:27 │ INFO     │     Resultados logreg: AUC=0.7573, Bal. Acc.=0.7018\n",
      "01:22:27 │ INFO     │   --- Fold 1/5, Clasificador: mlp ---\n",
      "01:22:27 │ INFO     │ Ajustando HPs para mlp con GridSearchCV...\n",
      "01:22:27 │ INFO     │ Mejores HPs encontrados para mlp: {'alpha': 0.1, 'hidden_layer_sizes': (128, 64), 'learning_rate_init': 0.005}\n",
      "01:22:27 │ INFO     │     Resultados mlp: AUC=0.8421, Bal. Acc.=0.7266\n",
      "01:22:27 │ INFO     │ --- Fold 1/5 completado en 405.63 segundos. ---\n",
      "01:22:27 │ INFO     │ --- Iniciando Fold 2/5 ---\n",
      "01:22:27 │ INFO     │ Aplicando normalización inter-canal (modo: zscore_offdiag) sobre 3 canales seleccionados.\n",
      "01:22:27 │ INFO     │ Parámetros de normalización se calcularán usando 334 sujetos de entrenamiento.\n",
      "01:22:27 │ INFO     │ Canal 'Pearson_Full_FisherZ_Signed': Off-diag zscore_offdiag (train_params: mean=-0.050, std=0.778)\n",
      "01:22:27 │ INFO     │ Canal 'MI_KNN_Symmetric': Off-diag zscore_offdiag (train_params: mean=0.056, std=0.816)\n",
      "01:22:27 │ INFO     │ Canal 'dFC_AbsDiffMean': Off-diag zscore_offdiag (train_params: mean=-0.037, std=0.769)\n",
      "/home/diego/anaconda3/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "01:22:28 │ INFO     │   Fold 2/5 VAE Ep   1 | LR 1.00e-04 | β 0.000 | Train Recon 63905.30  KLD   190.84 | Val Loss 50067.78\n",
      "01:22:35 │ INFO     │   Fold 2/5 VAE Ep  21 | LR 1.00e-04 | β 0.121 | Train Recon 40982.20  KLD  1245.62 | Val Loss 37147.43\n",
      "01:22:42 │ INFO     │   Fold 2/5 VAE Ep  41 | LR 1.00e-04 | β 0.242 | Train Recon 36062.69  KLD  1140.24 | Val Loss 32920.43\n",
      "01:22:48 │ INFO     │   Fold 2/5 VAE Ep  61 | LR 1.00e-04 | β 0.364 | Train Recon 33920.45  KLD  1034.16 | Val Loss 31288.36\n",
      "01:23:21 │ INFO     │   Fold 2/5 VAE Ep  81 | LR 1.00e-04 | β 0.485 | Train Recon 32638.01  KLD   940.71 | Val Loss 30318.85\n",
      "01:23:27 │ INFO     │   Fold 2/5 VAE Ep 101 | LR 1.00e-04 | β 0.606 | Train Recon 31651.51  KLD   882.55 | Val Loss 29715.52\n",
      "01:23:46 │ INFO     │   Fold 2/5 VAE Ep 121 | LR 1.00e-04 | β 0.727 | Train Recon 31022.30  KLD   825.23 | Val Loss 29356.93\n",
      "01:24:01 │ INFO     │   Fold 2/5 VAE Ep 141 | LR 1.00e-04 | β 0.848 | Train Recon 30475.25  KLD   778.85 | Val Loss 29014.53\n",
      "01:24:23 │ INFO     │   Fold 2/5 VAE Ep 161 | LR 1.00e-04 | β 0.970 | Train Recon 29961.42  KLD   734.34 | Val Loss 28800.57\n",
      "01:24:36 │ INFO     │   Fold 2/5 VAE Ep 181 | LR 1.00e-04 | β 1.091 | Train Recon 29526.03  KLD   705.44 | Val Loss 28511.37\n",
      "01:24:46 │ INFO     │   Fold 2/5 VAE Ep 201 | LR 1.00e-04 | β 1.212 | Train Recon 29186.54  KLD   667.31 | Val Loss 28250.93\n",
      "01:24:55 │ INFO     │   Fold 2/5 VAE Ep 221 | LR 1.00e-04 | β 1.333 | Train Recon 28883.64  KLD   646.91 | Val Loss 28192.03\n",
      "01:25:09 │ INFO     │   Fold 2/5 VAE Ep 241 | LR 1.00e-04 | β 1.455 | Train Recon 28634.80  KLD   623.75 | Val Loss 28126.45\n",
      "01:25:12 │ INFO     │   Fold 2/5 VAE Ep 261 | LR 1.00e-04 | β 1.500 | Train Recon 28526.34  KLD   606.31 | Val Loss 28105.12\n",
      "01:25:17 │ INFO     │   Fold 2/5 VAE Ep 281 | LR 1.00e-04 | β 0.030 | Train Recon 27962.12  KLD  1282.26 | Val Loss 26819.09\n",
      "01:25:23 │ INFO     │   Fold 2/5 VAE Ep 301 | LR 1.00e-04 | β 0.152 | Train Recon 27727.59  KLD  1250.31 | Val Loss 26885.93\n",
      "01:25:26 │ INFO     │   Fold 2/5 VAE Ep 321 | LR 1.00e-05 | β 0.273 | Train Recon 27463.63  KLD  1138.45 | Val Loss 26857.83\n",
      "01:25:28 │ INFO     │   Fold 2/5 Early stopping VAE en epoch 336.\n",
      "/home/diego/Escritorio/AAL3_paper/sat_night.py:676: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  vae_fold_k.load_state_dict(torch.load(fold_output_dir / \"best_vae_model.pt\"))\n",
      "01:25:28 │ INFO     │   Fold 2/5 Extrayendo features latentes con el VAE entrenado.\n",
      "01:25:28 │ INFO     │   --- Fold 2/5, Clasificador: rf ---\n",
      "01:25:28 │ INFO     │ Ajustando HPs para rf con Optuna (150 trials)...\n",
      "01:25:47 │ INFO     │ Mejores HPs encontrados para rf: {'n_estimators': 111, 'max_depth': 19, 'min_samples_split': 8, 'min_samples_leaf': 3}\n",
      "01:25:47 │ INFO     │     Resultados rf: AUC=0.7164, Bal. Acc.=0.6725\n",
      "01:25:47 │ INFO     │   --- Fold 2/5, Clasificador: gb ---\n",
      "01:25:47 │ INFO     │ Ajustando HPs para gb con Optuna (150 trials)...\n",
      "01:29:10 │ INFO     │ Mejores HPs encontrados para gb: {'n_estimators': 179, 'learning_rate': 0.09358130769576406, 'max_depth': 3}\n",
      "01:29:11 │ INFO     │     Resultados gb: AUC=0.6550, Bal. Acc.=0.6213\n",
      "01:29:11 │ INFO     │   --- Fold 2/5, Clasificador: svm ---\n",
      "01:29:11 │ INFO     │ Ajustando HPs para svm con GridSearchCV...\n",
      "01:29:12 │ INFO     │ Mejores HPs encontrados para svm: {'C': 10, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "01:29:12 │ INFO     │     Resultados svm: AUC=0.7661, Bal. Acc.=0.5965\n",
      "01:29:12 │ INFO     │   --- Fold 2/5, Clasificador: logreg ---\n",
      "01:29:12 │ INFO     │ Ajustando HPs para logreg con GridSearchCV...\n",
      "01:29:12 │ INFO     │ Mejores HPs encontrados para logreg: {'C': 0.1}\n",
      "01:29:12 │ INFO     │     Resultados logreg: AUC=0.7485, Bal. Acc.=0.6243\n",
      "01:29:12 │ INFO     │   --- Fold 2/5, Clasificador: mlp ---\n",
      "01:29:12 │ INFO     │ Ajustando HPs para mlp con GridSearchCV...\n",
      "01:29:12 │ INFO     │ Mejores HPs encontrados para mlp: {'alpha': 0.001, 'hidden_layer_sizes': (64,), 'learning_rate_init': 0.005}\n",
      "01:29:12 │ INFO     │     Resultados mlp: AUC=0.7632, Bal. Acc.=0.7018\n",
      "01:29:12 │ INFO     │ --- Fold 2/5 completado en 405.45 segundos. ---\n",
      "01:29:12 │ INFO     │ --- Iniciando Fold 3/5 ---\n",
      "01:29:12 │ INFO     │ Aplicando normalización inter-canal (modo: zscore_offdiag) sobre 3 canales seleccionados.\n",
      "01:29:12 │ INFO     │ Parámetros de normalización se calcularán usando 334 sujetos de entrenamiento.\n",
      "01:29:13 │ INFO     │ Canal 'Pearson_Full_FisherZ_Signed': Off-diag zscore_offdiag (train_params: mean=-0.047, std=0.775)\n",
      "01:29:13 │ INFO     │ Canal 'MI_KNN_Symmetric': Off-diag zscore_offdiag (train_params: mean=0.058, std=0.812)\n",
      "01:29:13 │ INFO     │ Canal 'dFC_AbsDiffMean': Off-diag zscore_offdiag (train_params: mean=-0.036, std=0.766)\n",
      "/home/diego/anaconda3/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "01:29:13 │ INFO     │   Fold 3/5 VAE Ep   1 | LR 1.00e-04 | β 0.000 | Train Recon 63660.09  KLD   189.00 | Val Loss 52836.11\n",
      "01:29:20 │ INFO     │   Fold 3/5 VAE Ep  21 | LR 1.00e-04 | β 0.121 | Train Recon 41131.34  KLD  1229.25 | Val Loss 39168.20\n",
      "01:29:26 │ INFO     │   Fold 3/5 VAE Ep  41 | LR 1.00e-04 | β 0.242 | Train Recon 35899.20  KLD  1108.06 | Val Loss 34506.76\n",
      "01:29:36 │ INFO     │   Fold 3/5 VAE Ep  61 | LR 1.00e-04 | β 0.364 | Train Recon 33763.96  KLD  1005.51 | Val Loss 32737.28\n",
      "01:30:04 │ INFO     │   Fold 3/5 VAE Ep  81 | LR 1.00e-04 | β 0.485 | Train Recon 32505.86  KLD   934.50 | Val Loss 31888.10\n",
      "01:30:15 │ INFO     │   Fold 3/5 VAE Ep 101 | LR 1.00e-04 | β 0.606 | Train Recon 31524.64  KLD   872.46 | Val Loss 31281.12\n",
      "01:31:59 │ INFO     │   Fold 3/5 VAE Ep 121 | LR 1.00e-04 | β 0.727 | Train Recon 30842.03  KLD   819.19 | Val Loss 30760.34\n",
      "01:32:15 │ INFO     │   Fold 3/5 VAE Ep 141 | LR 1.00e-04 | β 0.848 | Train Recon 30349.30  KLD   767.03 | Val Loss 30510.45\n",
      "01:32:22 │ INFO     │   Fold 3/5 VAE Ep 161 | LR 1.00e-04 | β 0.970 | Train Recon 29826.38  KLD   729.31 | Val Loss 30129.45\n",
      "01:32:36 │ INFO     │   Fold 3/5 VAE Ep 181 | LR 1.00e-04 | β 1.091 | Train Recon 29381.61  KLD   689.35 | Val Loss 29988.47\n",
      "01:32:46 │ INFO     │   Fold 3/5 VAE Ep 201 | LR 1.00e-04 | β 1.212 | Train Recon 29122.51  KLD   660.07 | Val Loss 29818.29\n",
      "01:32:51 │ INFO     │   Fold 3/5 VAE Ep 221 | LR 1.00e-04 | β 1.333 | Train Recon 28816.49  KLD   636.76 | Val Loss 29762.58\n",
      "01:32:54 │ INFO     │   Fold 3/5 VAE Ep 241 | LR 1.00e-04 | β 1.455 | Train Recon 28591.53  KLD   608.22 | Val Loss 29666.22\n",
      "01:32:59 │ INFO     │   Fold 3/5 VAE Ep 261 | LR 1.00e-04 | β 1.500 | Train Recon 28334.24  KLD   599.34 | Val Loss 29543.90\n",
      "01:33:12 │ INFO     │   Fold 3/5 VAE Ep 281 | LR 1.00e-04 | β 0.030 | Train Recon 27828.08  KLD  1300.08 | Val Loss 28392.44\n",
      "01:33:21 │ INFO     │   Fold 3/5 VAE Ep 301 | LR 1.00e-04 | β 0.152 | Train Recon 27530.94  KLD  1253.37 | Val Loss 28383.64\n",
      "01:33:24 │ INFO     │   Fold 3/5 VAE Ep 321 | LR 1.00e-05 | β 0.273 | Train Recon 27398.37  KLD  1125.46 | Val Loss 28439.97\n",
      "01:33:27 │ INFO     │   Fold 3/5 Early stopping VAE en epoch 340.\n",
      "/home/diego/Escritorio/AAL3_paper/sat_night.py:676: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  vae_fold_k.load_state_dict(torch.load(fold_output_dir / \"best_vae_model.pt\"))\n",
      "01:33:27 │ INFO     │   Fold 3/5 Extrayendo features latentes con el VAE entrenado.\n",
      "01:33:28 │ INFO     │   --- Fold 3/5, Clasificador: rf ---\n",
      "01:33:28 │ INFO     │ Ajustando HPs para rf con Optuna (150 trials)...\n",
      "01:33:47 │ INFO     │ Mejores HPs encontrados para rf: {'n_estimators': 137, 'max_depth': 19, 'min_samples_split': 5, 'min_samples_leaf': 1}\n",
      "01:33:48 │ INFO     │     Resultados rf: AUC=0.7047, Bal. Acc.=0.6711\n",
      "01:33:48 │ INFO     │   --- Fold 3/5, Clasificador: gb ---\n",
      "01:33:48 │ INFO     │ Ajustando HPs para gb con Optuna (150 trials)...\n",
      "01:37:08 │ INFO     │ Mejores HPs encontrados para gb: {'n_estimators': 64, 'learning_rate': 0.03981692768936942, 'max_depth': 4}\n",
      "01:37:09 │ INFO     │     Resultados gb: AUC=0.7719, Bal. Acc.=0.7295\n",
      "01:37:09 │ INFO     │   --- Fold 3/5, Clasificador: svm ---\n",
      "01:37:09 │ INFO     │ Ajustando HPs para svm con GridSearchCV...\n",
      "01:37:09 │ INFO     │ Mejores HPs encontrados para svm: {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "01:37:09 │ INFO     │     Resultados svm: AUC=0.7222, Bal. Acc.=0.6506\n",
      "01:37:09 │ INFO     │   --- Fold 3/5, Clasificador: logreg ---\n",
      "01:37:09 │ INFO     │ Ajustando HPs para logreg con GridSearchCV...\n",
      "01:37:09 │ INFO     │ Mejores HPs encontrados para logreg: {'C': 0.01}\n",
      "01:37:09 │ INFO     │     Resultados logreg: AUC=0.7310, Bal. Acc.=0.6769\n",
      "01:37:09 │ INFO     │   --- Fold 3/5, Clasificador: mlp ---\n",
      "01:37:09 │ INFO     │ Ajustando HPs para mlp con GridSearchCV...\n",
      "01:37:10 │ INFO     │ Mejores HPs encontrados para mlp: {'alpha': 0.1, 'hidden_layer_sizes': (128, 64), 'learning_rate_init': 0.005}\n",
      "01:37:10 │ INFO     │     Resultados mlp: AUC=0.6784, Bal. Acc.=0.6213\n",
      "01:37:10 │ INFO     │ --- Fold 3/5 completado en 477.40 segundos. ---\n",
      "01:37:10 │ INFO     │ --- Iniciando Fold 4/5 ---\n",
      "01:37:10 │ INFO     │ Aplicando normalización inter-canal (modo: zscore_offdiag) sobre 3 canales seleccionados.\n",
      "01:37:10 │ INFO     │ Parámetros de normalización se calcularán usando 334 sujetos de entrenamiento.\n",
      "01:37:10 │ INFO     │ Canal 'Pearson_Full_FisherZ_Signed': Off-diag zscore_offdiag (train_params: mean=-0.049, std=0.777)\n",
      "01:37:10 │ INFO     │ Canal 'MI_KNN_Symmetric': Off-diag zscore_offdiag (train_params: mean=0.055, std=0.812)\n",
      "01:37:10 │ INFO     │ Canal 'dFC_AbsDiffMean': Off-diag zscore_offdiag (train_params: mean=-0.037, std=0.768)\n",
      "/home/diego/anaconda3/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "01:37:10 │ INFO     │   Fold 4/5 VAE Ep   1 | LR 1.00e-04 | β 0.000 | Train Recon 64166.47  KLD   183.92 | Val Loss 51556.98\n",
      "01:37:17 │ INFO     │   Fold 4/5 VAE Ep  21 | LR 1.00e-04 | β 0.121 | Train Recon 41515.55  KLD  1238.01 | Val Loss 39689.28\n",
      "01:37:23 │ INFO     │   Fold 4/5 VAE Ep  41 | LR 1.00e-04 | β 0.242 | Train Recon 36354.39  KLD  1082.42 | Val Loss 35201.99\n",
      "01:37:37 │ INFO     │   Fold 4/5 VAE Ep  61 | LR 1.00e-04 | β 0.364 | Train Recon 34160.74  KLD   996.69 | Val Loss 33237.43\n",
      "01:38:06 │ INFO     │   Fold 4/5 VAE Ep  81 | LR 1.00e-04 | β 0.485 | Train Recon 32801.26  KLD   912.39 | Val Loss 32200.51\n",
      "01:38:33 │ INFO     │   Fold 4/5 VAE Ep 101 | LR 1.00e-04 | β 0.606 | Train Recon 31763.92  KLD   862.22 | Val Loss 31640.63\n",
      "01:38:39 │ INFO     │   Fold 4/5 VAE Ep 121 | LR 1.00e-04 | β 0.727 | Train Recon 31118.55  KLD   812.08 | Val Loss 31095.93\n",
      "01:38:44 │ INFO     │   Fold 4/5 VAE Ep 141 | LR 1.00e-04 | β 0.848 | Train Recon 30513.68  KLD   770.87 | Val Loss 30827.60\n",
      "01:38:48 │ INFO     │   Fold 4/5 VAE Ep 161 | LR 1.00e-04 | β 0.970 | Train Recon 29972.88  KLD   736.01 | Val Loss 30428.61\n",
      "01:39:00 │ INFO     │   Fold 4/5 VAE Ep 181 | LR 1.00e-04 | β 1.091 | Train Recon 29484.34  KLD   694.78 | Val Loss 30143.36\n",
      "01:39:14 │ INFO     │   Fold 4/5 VAE Ep 201 | LR 1.00e-04 | β 1.212 | Train Recon 29203.19  KLD   664.27 | Val Loss 30052.33\n",
      "01:39:22 │ INFO     │   Fold 4/5 VAE Ep 221 | LR 1.00e-04 | β 1.333 | Train Recon 28831.15  KLD   642.13 | Val Loss 29844.74\n",
      "01:39:34 │ INFO     │   Fold 4/5 VAE Ep 241 | LR 1.00e-04 | β 1.455 | Train Recon 28691.62  KLD   624.71 | Val Loss 29864.56\n",
      "01:39:47 │ INFO     │   Fold 4/5 VAE Ep 261 | LR 1.00e-04 | β 1.500 | Train Recon 28417.71  KLD   597.16 | Val Loss 29573.13\n",
      "01:39:56 │ INFO     │   Fold 4/5 VAE Ep 281 | LR 1.00e-04 | β 0.030 | Train Recon 27905.75  KLD  1280.94 | Val Loss 28479.99\n",
      "01:40:01 │ INFO     │   Fold 4/5 VAE Ep 301 | LR 1.00e-04 | β 0.152 | Train Recon 27689.21  KLD  1243.73 | Val Loss 28509.21\n",
      "01:40:04 │ INFO     │   Fold 4/5 VAE Ep 321 | LR 1.00e-05 | β 0.273 | Train Recon 27451.00  KLD  1125.72 | Val Loss 28492.76\n",
      "01:40:07 │ INFO     │   Fold 4/5 Early stopping VAE en epoch 339.\n",
      "/home/diego/Escritorio/AAL3_paper/sat_night.py:676: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  vae_fold_k.load_state_dict(torch.load(fold_output_dir / \"best_vae_model.pt\"))\n",
      "01:40:07 │ INFO     │   Fold 4/5 Extrayendo features latentes con el VAE entrenado.\n",
      "01:40:07 │ INFO     │   --- Fold 4/5, Clasificador: rf ---\n",
      "01:40:07 │ INFO     │ Ajustando HPs para rf con Optuna (150 trials)...\n",
      "01:40:32 │ INFO     │ Mejores HPs encontrados para rf: {'n_estimators': 228, 'max_depth': 18, 'min_samples_split': 2, 'min_samples_leaf': 6}\n",
      "01:40:32 │ INFO     │     Resultados rf: AUC=0.7778, Bal. Acc.=0.7295\n",
      "01:40:32 │ INFO     │   --- Fold 4/5, Clasificador: gb ---\n",
      "01:40:32 │ INFO     │ Ajustando HPs para gb con Optuna (150 trials)...\n",
      "01:44:41 │ INFO     │ Mejores HPs encontrados para gb: {'n_estimators': 236, 'learning_rate': 0.15977076402838497, 'max_depth': 4}\n",
      "01:44:43 │ INFO     │     Resultados gb: AUC=0.7661, Bal. Acc.=0.7266\n",
      "01:44:43 │ INFO     │   --- Fold 4/5, Clasificador: svm ---\n",
      "01:44:43 │ INFO     │ Ajustando HPs para svm con GridSearchCV...\n",
      "01:44:43 │ INFO     │ Mejores HPs encontrados para svm: {'C': 10, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "01:44:43 │ INFO     │     Resultados svm: AUC=0.8129, Bal. Acc.=0.7851\n",
      "01:44:43 │ INFO     │   --- Fold 4/5, Clasificador: logreg ---\n",
      "01:44:43 │ INFO     │ Ajustando HPs para logreg con GridSearchCV...\n",
      "01:44:43 │ INFO     │ Mejores HPs encontrados para logreg: {'C': 0.01}\n",
      "01:44:43 │ INFO     │     Resultados logreg: AUC=0.8099, Bal. Acc.=0.8114\n",
      "01:44:43 │ INFO     │   --- Fold 4/5, Clasificador: mlp ---\n",
      "01:44:43 │ INFO     │ Ajustando HPs para mlp con GridSearchCV...\n",
      "01:44:44 │ INFO     │ Mejores HPs encontrados para mlp: {'alpha': 0.001, 'hidden_layer_sizes': (64,), 'learning_rate_init': 0.005}\n",
      "01:44:44 │ INFO     │     Resultados mlp: AUC=0.7924, Bal. Acc.=0.7032\n",
      "01:44:44 │ INFO     │ --- Fold 4/5 completado en 453.91 segundos. ---\n",
      "01:44:44 │ INFO     │ --- Iniciando Fold 5/5 ---\n",
      "01:44:44 │ INFO     │ Aplicando normalización inter-canal (modo: zscore_offdiag) sobre 3 canales seleccionados.\n",
      "01:44:44 │ INFO     │ Parámetros de normalización se calcularán usando 335 sujetos de entrenamiento.\n",
      "01:44:44 │ INFO     │ Canal 'Pearson_Full_FisherZ_Signed': Off-diag zscore_offdiag (train_params: mean=-0.047, std=0.777)\n",
      "01:44:44 │ INFO     │ Canal 'MI_KNN_Symmetric': Off-diag zscore_offdiag (train_params: mean=0.056, std=0.812)\n",
      "01:44:44 │ INFO     │ Canal 'dFC_AbsDiffMean': Off-diag zscore_offdiag (train_params: mean=-0.036, std=0.764)\n",
      "/home/diego/anaconda3/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "01:44:44 │ INFO     │   Fold 5/5 VAE Ep   1 | LR 1.00e-04 | β 0.000 | Train Recon 63622.24  KLD   190.31 | Val Loss 53245.24\n",
      "01:44:51 │ INFO     │   Fold 5/5 VAE Ep  21 | LR 1.00e-04 | β 0.121 | Train Recon 40959.69  KLD  1213.48 | Val Loss 40587.06\n",
      "01:44:57 │ INFO     │   Fold 5/5 VAE Ep  41 | LR 1.00e-04 | β 0.242 | Train Recon 36198.59  KLD  1105.88 | Val Loss 36324.86\n",
      "01:45:04 │ INFO     │   Fold 5/5 VAE Ep  61 | LR 1.00e-04 | β 0.364 | Train Recon 33892.41  KLD   992.81 | Val Loss 34510.64\n",
      "01:45:31 │ INFO     │   Fold 5/5 VAE Ep  81 | LR 1.00e-04 | β 0.485 | Train Recon 32610.04  KLD   916.45 | Val Loss 33569.32\n",
      "01:45:40 │ INFO     │   Fold 5/5 VAE Ep 101 | LR 1.00e-04 | β 0.606 | Train Recon 31619.04  KLD   868.71 | Val Loss 32826.16\n",
      "01:45:56 │ INFO     │   Fold 5/5 VAE Ep 121 | LR 1.00e-04 | β 0.727 | Train Recon 30880.63  KLD   801.62 | Val Loss 32373.98\n",
      "01:46:14 │ INFO     │   Fold 5/5 VAE Ep 141 | LR 1.00e-04 | β 0.848 | Train Recon 30387.70  KLD   767.40 | Val Loss 32139.47\n",
      "01:46:28 │ INFO     │   Fold 5/5 VAE Ep 161 | LR 1.00e-04 | β 0.970 | Train Recon 29883.07  KLD   737.20 | Val Loss 31922.36\n",
      "01:46:43 │ INFO     │   Fold 5/5 VAE Ep 181 | LR 1.00e-04 | β 1.091 | Train Recon 29525.05  KLD   694.61 | Val Loss 31556.37\n",
      "01:46:53 │ INFO     │   Fold 5/5 VAE Ep 201 | LR 1.00e-04 | β 1.212 | Train Recon 29096.28  KLD   666.73 | Val Loss 31474.47\n",
      "01:47:01 │ INFO     │   Fold 5/5 VAE Ep 221 | LR 1.00e-04 | β 1.333 | Train Recon 28901.25  KLD   637.38 | Val Loss 31269.17\n",
      "01:47:11 │ INFO     │   Fold 5/5 VAE Ep 241 | LR 1.00e-04 | β 1.455 | Train Recon 28706.99  KLD   620.63 | Val Loss 31163.50\n",
      "01:47:15 │ INFO     │   Fold 5/5 VAE Ep 261 | LR 1.00e-04 | β 1.500 | Train Recon 28455.59  KLD   602.56 | Val Loss 31093.35\n",
      "01:47:34 │ INFO     │   Fold 5/5 VAE Ep 281 | LR 1.00e-04 | β 0.030 | Train Recon 27881.11  KLD  1272.66 | Val Loss 29925.21\n",
      "01:47:37 │ INFO     │   Fold 5/5 VAE Ep 301 | LR 1.00e-05 | β 0.152 | Train Recon 27721.24  KLD  1257.48 | Val Loss 29961.04\n",
      "01:47:41 │ INFO     │   Fold 5/5 VAE Ep 321 | LR 1.00e-05 | β 0.273 | Train Recon 27520.52  KLD  1176.25 | Val Loss 29984.93\n",
      "01:47:42 │ INFO     │   Fold 5/5 Early stopping VAE en epoch 330.\n",
      "/home/diego/Escritorio/AAL3_paper/sat_night.py:676: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  vae_fold_k.load_state_dict(torch.load(fold_output_dir / \"best_vae_model.pt\"))\n",
      "01:47:42 │ INFO     │   Fold 5/5 Extrayendo features latentes con el VAE entrenado.\n",
      "01:47:42 │ INFO     │   --- Fold 5/5, Clasificador: rf ---\n",
      "01:47:42 │ INFO     │ Ajustando HPs para rf con Optuna (150 trials)...\n",
      "01:47:56 │ INFO     │ Mejores HPs encontrados para rf: {'n_estimators': 53, 'max_depth': 6, 'min_samples_split': 5, 'min_samples_leaf': 9}\n",
      "01:47:56 │ INFO     │     Resultados rf: AUC=0.5759, Bal. Acc.=0.4954\n",
      "01:47:56 │ INFO     │   --- Fold 5/5, Clasificador: gb ---\n",
      "01:47:56 │ INFO     │ Ajustando HPs para gb con Optuna (150 trials)...\n",
      "01:51:10 │ INFO     │ Mejores HPs encontrados para gb: {'n_estimators': 131, 'learning_rate': 0.018443362328297012, 'max_depth': 3}\n",
      "01:51:11 │ INFO     │     Resultados gb: AUC=0.5728, Bal. Acc.=0.5805\n",
      "01:51:11 │ INFO     │   --- Fold 5/5, Clasificador: svm ---\n",
      "01:51:11 │ INFO     │ Ajustando HPs para svm con GridSearchCV...\n",
      "01:51:12 │ INFO     │ Mejores HPs encontrados para svm: {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "01:51:12 │ INFO     │     Resultados svm: AUC=0.6718, Bal. Acc.=0.6161\n",
      "01:51:12 │ INFO     │   --- Fold 5/5, Clasificador: logreg ---\n",
      "01:51:12 │ INFO     │ Ajustando HPs para logreg con GridSearchCV...\n",
      "01:51:12 │ INFO     │ Mejores HPs encontrados para logreg: {'C': 0.1}\n",
      "01:51:12 │ INFO     │     Resultados logreg: AUC=0.7307, Bal. Acc.=0.6594\n",
      "01:51:12 │ INFO     │   --- Fold 5/5, Clasificador: mlp ---\n",
      "01:51:12 │ INFO     │ Ajustando HPs para mlp con GridSearchCV...\n",
      "01:51:12 │ INFO     │ Mejores HPs encontrados para mlp: {'alpha': 0.001, 'hidden_layer_sizes': (128, 64), 'learning_rate_init': 0.005}\n",
      "01:51:12 │ INFO     │     Resultados mlp: AUC=0.5666, Bal. Acc.=0.6037\n",
      "01:51:12 │ INFO     │ --- Fold 5/5 completado en 388.33 segundos. ---\n",
      "01:51:12 │ INFO     │ \n",
      "--- RESUMEN FINAL (Promedio +/- STD sobre Folds) ---\n",
      "01:51:12 │ INFO     │ \n",
      "  Clasificador: rf\n",
      "01:51:12 │ INFO     │     Auc                 : 0.7251 +/- 0.1017\n",
      "01:51:12 │ INFO     │     Pr_auc              : 0.7482 +/- 0.1043\n",
      "01:51:12 │ INFO     │     Accuracy            : 0.6622 +/- 0.0946\n",
      "01:51:12 │ INFO     │     Balanced_accuracy   : 0.6590 +/- 0.0957\n",
      "01:51:12 │ INFO     │     F1_score            : 0.6981 +/- 0.0846\n",
      "01:51:12 │ INFO     │     Sensitivity         : 0.7579 +/- 0.1091\n",
      "01:51:12 │ INFO     │     Specificity         : 0.5601 +/- 0.1167\n",
      "01:51:12 │ INFO     │ \n",
      "  Clasificador: gb\n",
      "01:51:12 │ INFO     │     Auc                 : 0.7099 +/- 0.0926\n",
      "01:51:12 │ INFO     │     Pr_auc              : 0.7309 +/- 0.0876\n",
      "01:51:12 │ INFO     │     Accuracy            : 0.6680 +/- 0.0652\n",
      "01:51:12 │ INFO     │     Balanced_accuracy   : 0.6667 +/- 0.0654\n",
      "01:51:12 │ INFO     │     F1_score            : 0.6860 +/- 0.0638\n",
      "01:51:12 │ INFO     │     Sensitivity         : 0.7053 +/- 0.0881\n",
      "01:51:12 │ INFO     │     Specificity         : 0.6281 +/- 0.0719\n",
      "01:51:12 │ INFO     │ \n",
      "  Clasificador: svm\n",
      "01:51:12 │ INFO     │     Auc                 : 0.7695 +/- 0.0785\n",
      "01:51:12 │ INFO     │     Pr_auc              : 0.7803 +/- 0.0973\n",
      "01:51:12 │ INFO     │     Accuracy            : 0.6844 +/- 0.0928\n",
      "01:51:12 │ INFO     │     Balanced_accuracy   : 0.6864 +/- 0.0915\n",
      "01:51:12 │ INFO     │     F1_score            : 0.6711 +/- 0.1049\n",
      "01:51:12 │ INFO     │     Sensitivity         : 0.6316 +/- 0.1234\n",
      "01:51:12 │ INFO     │     Specificity         : 0.7412 +/- 0.0652\n",
      "01:51:12 │ INFO     │ \n",
      "  Clasificador: logreg\n",
      "01:51:12 │ INFO     │     Auc                 : 0.7555 +/- 0.0325\n",
      "01:51:12 │ INFO     │     Pr_auc              : 0.7638 +/- 0.0542\n",
      "01:51:12 │ INFO     │     Accuracy            : 0.6955 +/- 0.0708\n",
      "01:51:12 │ INFO     │     Balanced_accuracy   : 0.6948 +/- 0.0710\n",
      "01:51:12 │ INFO     │     F1_score            : 0.6996 +/- 0.0813\n",
      "01:51:12 │ INFO     │     Sensitivity         : 0.6947 +/- 0.1141\n",
      "01:51:12 │ INFO     │     Specificity         : 0.6948 +/- 0.1105\n",
      "01:51:12 │ INFO     │ \n",
      "  Clasificador: mlp\n",
      "01:51:12 │ INFO     │     Auc                 : 0.7285 +/- 0.1083\n",
      "01:51:12 │ INFO     │     Pr_auc              : 0.7496 +/- 0.1051\n",
      "01:51:12 │ INFO     │     Accuracy            : 0.6736 +/- 0.0535\n",
      "01:51:12 │ INFO     │     Balanced_accuracy   : 0.6713 +/- 0.0549\n",
      "01:51:12 │ INFO     │     F1_score            : 0.6962 +/- 0.0497\n",
      "01:51:12 │ INFO     │     Sensitivity         : 0.7263 +/- 0.0781\n",
      "01:51:12 │ INFO     │     Specificity         : 0.6163 +/- 0.0936\n",
      "01:51:12 │ INFO     │ \n",
      "Resultados detallados de todos los folds guardados en: experiments/wed_night_run/final_classification_metrics.csv\n"
     ]
    }
   ],
   "source": [
    "!python sat_night.py \\\n",
    "    --global_tensor_path /home/diego/Escritorio/AAL3_paper/AAL3_dynamicROIs_fmri_tensor_NeuroEnhanced_v6.5.17_AAL3_131ROIs_OMST_GCE_Signed_GrangerLag1_ChNorm_ROIreorderedYeo17_ParallelTuned/GLOBAL_TENSOR_from_AAL3_dynamicROIs_fmri_tensor_NeuroEnhanced_v6.5.17_AAL3_131ROIs_OMST_GCE_Signed_GrangerLag1_ChNorm_ROIreorderedYeo17_ParallelTuned.npz \\\n",
    "    --metadata_path      /home/diego/Escritorio/AAL3_paper/SubjectsData_Schaefer2018_400ROIs.csv \\\n",
    "    --channels_to_use 1 2 3 \\\n",
    "    --outer_folds 5 --repeated_outer_folds_n_repeats 1 \\\n",
    "    --classifier_stratify_cols Sex \\\n",
    "    --classifier_hp_tune_ratio 0.2 \\\n",
    "    --latent_dim 512 --num_conv_layers_encoder 4 --decoder_type convtranspose \\\n",
    "    --epochs_vae 550 --lr_vae 1e-4 --weight_decay_vae 1e-4 \\\n",
    "    --batch_size 32 --beta_vae 1.2 \\\n",
    "    --cyclical_beta_n_cycles 2 --cyclical_beta_ratio_increase 0.9 \\\n",
    "    --vae_val_split_ratio 0.15 --early_stopping_patience_vae 50 \\\n",
    "    --lr_scheduler_patience_vae 20 \\\n",
    "    --dropout_rate_vae 0.2 --use_layernorm_vae_fc \\\n",
    "    --classifier_types rf gb svm logreg mlp \\\n",
    "    --classifier_use_class_weight \\\n",
    "    --norm_mode zscore_offdiag \\\n",
    "    --n_jobs_gridsearch 4 \\\n",
    "    --optuna_n_trials 150 \\\n",
    "    --output_dir experiments/wed_night_run \\\n",
    "    --log_level INFO \\\n",
    "    --save_vae_training_history --save_fold_artefacts\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
